{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12/21/21**\n",
    "\n",
    "The purpose of this notebook is to document construction and refinement of the Community database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elliot_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacteriaDir = Path.cwd().joinpath('../PublicSequences/Combined_AllNCBI_12-21/')\n",
    "eukaryotesDir = Path.cwd().joinpath('../PublicSequences/Combined_AllNCBI_Eukaryotes_12-21/')\n",
    "humanFile = Path.cwd().joinpath('../PublicSequences/Human9606_2-6-2019_TrypPigBov.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get human and contaminant data from the file\n",
    "humanData = ''\n",
    "with humanFile.open(mode='r') as infile:\n",
    "    humanData = infile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate bacterial proteins\n",
    "bacteriaProteins = {} #key=protein seqeunce, value=protein object\n",
    "for fastafile in bacteriaDir.iterdir():\n",
    "    species = fastafile.stem.replace('_', ' ')\n",
    "    data = ''\n",
    "    with fastafile.open(mode='r') as infile:\n",
    "        data = infile.read()\n",
    "    dataList = data.split('\\n>')\n",
    "    dataList[0] = dataList[0][1:]\n",
    "    del data\n",
    "    for sequence in dataList:\n",
    "        newProt = Protein(sequence)\n",
    "        if newProt.sequence in bacteriaProteins.keys():\n",
    "            bacteriaProteins[newProt.sequence].addTaxa(newProt.taxa[0])\n",
    "        else:\n",
    "            bacteriaProteins[newProt.sequence] = newProt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate nonhuman eukaryotic proteins\n",
    "eukProteins = {} #key=protein seqeunce, value=protein object\n",
    "for fastafile in eukaryotesDir.iterdir():\n",
    "    species = fastafile.stem.replace('_', ' ')\n",
    "    data = ''\n",
    "    with fastafile.open(mode='r') as infile:\n",
    "        data = infile.read()\n",
    "    dataList = data.split('\\n>')\n",
    "    dataList[0] = dataList[0][1:]\n",
    "    del data\n",
    "    for sequence in dataList:\n",
    "        newProt = Protein(sequence)\n",
    "        if newProt.sequence in eukProteins.keys():\n",
    "            eukProteins[newProt.sequence].addTaxa(newProt.taxa[0])\n",
    "        else:\n",
    "            eukProteins[newProt.sequence] = newProt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbDir = Path.cwd().joinpath('../12-21-21_NextflowMSGF_Community5_Combined/databases/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by writing out human and eukaryotic proteins to a new fasta file\n",
    "dbIndex = 1\n",
    "dbPath = dbDir.joinpath(f'Community5_{str(dbIndex)}.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "humanEukToWrite = [humanData]\n",
    "for prot in eukProteins.values():\n",
    "    humanEukToWrite.append(prot.getEntry())\n",
    "with dbPath.open(mode='w', newline='') as dbfile:\n",
    "    dbfile.write(''.join(humanEukToWrite))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the bacterial proteins, keeping each file approximately 300mb in size\n",
    "bactToWrite = []\n",
    "for prot in bacteriaProteins.values():\n",
    "    bactToWrite.append(prot.getEntry())\n",
    "_iter = 0\n",
    "while _iter < len(bactToWrite):\n",
    "    dbPath = dbDir.joinpath(f'Community5_{str(dbIndex)}.fasta')\n",
    "    with dbPath.open(mode='a', newline='') as outfile:\n",
    "        for i in range(_iter, len(bactToWrite)):\n",
    "            outfile.write(bactToWrite[i])\n",
    "            _iter += 1\n",
    "            if dbPath.stat().st_size > 300000000:\n",
    "                dbIndex += 1\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condense the results down into a single file for each sample\n",
    "resultsDir = Path.cwd().joinpath('../12-21-21_NextflowMSGF_Community5_Combined/output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done sorting files\n",
      "Done condensing file 1284\n",
      "Done condensing file 1289\n",
      "Done condensing file 1290\n",
      "Done condensing file 1292\n",
      "Done condensing file 1294\n",
      "Done condensing file 1296\n",
      "Done condensing file 1299\n",
      "Done condensing file 1303\n",
      "Done condensing file 1304\n",
      "Done condensing file 1306\n",
      "Done condensing file 1310\n",
      "Done condensing file 1314\n",
      "Done condensing file 1316\n",
      "Done condensing file 1318\n",
      "Done condensing file 1320\n",
      "Done condensing file 1322\n",
      "Done condensing file 1324\n",
      "Done condensing file 1326\n",
      "Done condensing file 1328\n",
      "Done condensing file 1334\n",
      "Done condensing file 1336\n",
      "Done condensing file 1338\n",
      "Done condensing file 1340\n",
      "Done condensing file 1342\n",
      "Done condensing file 1346\n",
      "Done condensing file 1348\n",
      "Done condensing file 1350\n",
      "Done condensing file 1356\n",
      "Done condensing file 1358\n"
     ]
    }
   ],
   "source": [
    "condensedResults = condenseHugeDBResults(resultsDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "condensedResultsDir = Path.cwd().joinpath('../12-21-21_NextflowMSGF_Community5_Combined/output_processed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idNumber, tsvFile in condensedResults.items():\n",
    "    newfilePath = condensedResultsDir.joinpath(f'{idNumber}-Combined_Fredricks_CVL_23Dec14_Pippin_14-08-21_dta_Community5.tsv')\n",
    "    tsvFile.writeToFile(newfilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refine the database\n",
    "hitProteins = getHitsInResults(condensedResultsDir)\n",
    "refinedDBDir = Path.cwd().joinpath('../12-21-21_NextflowMSGF_Community5_Combined/databases_refined/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done reading Community5_1.fasta\n",
      "Done reading Community5_2.fasta\n",
      "Done reading Community5_3.fasta\n",
      "846002 sequences written.\n"
     ]
    }
   ],
   "source": [
    "refineHugeDatabase(hitProteins, dbDir, refinedDBDir, 'Community5_Refined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputRefinedDir = Path.cwd().joinpath('../12-21-21_NextflowMSGF_Community5_Combined/output_refined/')\n",
    "condensedRefinedDir = Path.cwd().joinpath('../12-21-21_NextflowMSGF_Community5_Combined/output_refined_processed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done sorting files\n",
      "Done condensing file 1284\n",
      "Done condensing file 1289\n",
      "Done condensing file 1290\n",
      "Done condensing file 1292\n",
      "Done condensing file 1294\n",
      "Done condensing file 1296\n",
      "Done condensing file 1299\n",
      "Done condensing file 1303\n",
      "Done condensing file 1304\n",
      "Done condensing file 1306\n",
      "Done condensing file 1310\n",
      "Done condensing file 1314\n",
      "Done condensing file 1316\n",
      "Done condensing file 1318\n",
      "Done condensing file 1320\n",
      "Done condensing file 1322\n",
      "Done condensing file 1324\n",
      "Done condensing file 1326\n",
      "Done condensing file 1328\n",
      "Done condensing file 1334\n",
      "Done condensing file 1336\n",
      "Done condensing file 1338\n",
      "Done condensing file 1340\n",
      "Done condensing file 1342\n",
      "Done condensing file 1346\n",
      "Done condensing file 1348\n",
      "Done condensing file 1350\n",
      "Done condensing file 1356\n",
      "Done condensing file 1358\n"
     ]
    }
   ],
   "source": [
    "condensedRefinedResults = condenseHugeDBResults(outputRefinedDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idNumber, tsvFile in condensedRefinedResults.items():\n",
    "    newfilePath = condensedRefinedDir.joinpath(f'{idNumber}-Combined_Fredricks_CVL_23Dec14_Pippin_14-08-21_dta_Community5_Refined.tsv')\n",
    "    tsvFile.writeToFile(newfilePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because a single refined database incorporates hits from all searched samples, I need to make and search another refined Community database for the subset of samples to compare against global. I'll go through the same process as above, but this time using only the Subset of sample results to determine what proteins go into the refined database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputSubsetDir = Path.cwd().joinpath('../12-21-21_NextflowMSGF_Community5_Combined/output_subset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetHitProteins = getHitsInResults(outputSubsetDir)\n",
    "refinedSubsetDBDir = Path.cwd().joinpath('../12-21-21_NextflowMSGF_Community5_Combined/databases_subset_refined/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done reading Community5_1.fasta\n",
      "Done reading Community5_2.fasta\n",
      "Done reading Community5_3.fasta\n",
      "360252 sequences written.\n"
     ]
    }
   ],
   "source": [
    "refineHugeDatabase(subsetHitProteins, dbDir, refinedSubsetDBDir, 'Community5_Subset_Refined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the results of the refined subset database search, since that database was only a single file\n",
    "subsetRefinedResults = getOrderedFiles(Path.cwd().joinpath('../12-21-21_NextflowMSGF_Community5_Combined/output_subset_refined/'), '.tsv')\n",
    "subsetRefinedProcessedDir = Path.cwd().joinpath('../12-21-21_NextflowMSGF_Community5_Combined/output_subset_refined_processed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "collapseRepeatRows(subsetRefinedResults, subsetRefinedProcessedDir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
